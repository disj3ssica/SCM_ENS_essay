\documentclass[12pt,a4paper,draft]{article}
\usepackage{xcolor}
\usepackage{amsmath}

\begin{document}

\begin{titlepage}
\title{An introduction to Synthetic Control Methods and their applications to climate change analysis}
\author{Jessica Cremonese}
\date{March 2023}
\maketitle

\vspace{2cm}

\begin{center}
    École Normale Supérieure
\end{center}

\begin{center}
    Scuola Galileiana di Studi Superiori
\end{center}

\vspace{5cm}
Professor Marc Fleurbaey.


\end{titlepage}

\tableofcontents

%\newpage
%\begin{abstract}
%    \textcolor{red}{Lorem ipsum}
%\end{abstract}
\newpage


\section{Introduction} %to do

\textcolor{red}{Lorem ipsum}

sSCM termed by Athey and Imbens (2017) as ``arguably the most important innovation in the 
policy evaluation literature in the last 15 years".




\section{Synthetic Control Methods}

Synthetic Control Methods (SCM) have been originally proposed in Abadie and Gardeazabal 
(2003) and by Abadie et al. (2010) to estimate the effects of aggregate interventions.
The key idea behind the method is that, when units are a few aggregate entities, 
a better counterfactual than using any single unit can be derived by computing a 
combination of the untreated units that closely resembles the treated one, i.e. 
a ``synthetic control". The selection of the ``donor units" is formalized with a data
driven procedure.
Although the method was originally intended for samples with few units, it has been 
successfully applied in contexts with large samples, for instance in Acemoglu et 
al. (2016).
Such a synthetic control unit is computed as a weighted average of all potential 
comparison units that best resemble the treated units. A good synthetic unit will 
resemble treated one not only in terms of the outcomes variable (outer optimization), 
but also in terms of the pre-treatment predictors value (inner optimization).

In this section, I will introduce the method and explore feasibility, 
data requirements and methodological issues. 
The main references are Abadie and Gardeazabal (2003) and Abadie, Diamond and 
Hainmueller (2010), which introduced the method in the literature, and Abadie 
(2021), which provides a useful guide to the application of SCM.


\subsection{Setting the method}

Suppose to have data for $j=1,..,J+1$ units, and suppose that unit $j=1$ is the 
treated unit. The ``donor pool" of untreated units which will contribute to the 
construction of a synthetic control for unit $j=1$ is then constituted by the 
remaining $j=2,...,J+1$ units.
Assume that data covers $T$ periods, with periods up to $T_0$ being the 
pre-intervention observations.

For each unit $j$ at time $t$ data is available for the outcome of interest 
$Y_{jt}$, and for a number $k$ of predictors $X_{1j}, ..., X_{kj}$. Define the 
$k \times 1$ vectors $\mathbf{X}_1, ..., \mathbf{X}_{J+1}$ which contain values
of the predictors for units $j=1,...,J+1$. Define the $k \times J$ matrix 
$\mathbf{X}_0= \left(\mathbf{X}_2,..., \mathbf{X}_{J+1}\right)$ which collects values of the predictors 
or the untreated units.
For each unit $j$, define the potential outcome without treatment as $Y_{jt}^N$.
For the treated unit $j=1$, define the potential response under the treatment as 
$Y_{jt}^I$ in the post treatment period $t>T_0$. The effect of the intervetion 
for the affected unit $j=1$ for $t>T_0$ is: 
\begin{equation}
\tau_{1t}=Y_{1t}^I-Y_{1t}^N
\end{equation}

For the treated unit, $Y_{1t}^I$ is observed so that  $Y_{1t}=Y_{1t}^I$, but $Y_{jt}^N$ is not. 
SCM provides a way to estimate $Y_{jt}^N$ for $t>T_0$, that is, how the outcome 
of interest would have been in the absence of treatment. Notice that $\tau_{1t}$
is allowed to change over time.



\subsection{Estimation} %parte mancante su inferenza

A downside of comparative case studies lies in the attempt to select the control 
units by informally arguing for an affinity between the treated and the untreated 
before the intervention. However, when using data from aggregate units such as 
countries or regions, it can be difficult to find a proper counterfactual.
SCM offers a formal procedure to select and combine the comparison units in order
to create a synthetic counterfactual scenario where unit $j=1$ was unaffected by treatment.

Define $\mathbf{W}=w_2,...,w_{J+1}$ as a $J\times 1$ vector of nonnegative weights
that sum to one. 
The $\mathbf{W}$ vector attributes a weight to each pre-treatment unit in the donor pool 
$j=2,...,J+1$ and characterizes its contribution to the synthetic unit. 

For a set of weights, $\mathbf{W}$, the estimators of $Y_{1t}^N$ and $\tau_{1t}$ are:
\begin{equation}
    \hat{Y}_{1t}^N = \sum_{j=2}^{J+1} {w_jY_{jt}}
\end{equation}

\begin{equation}
    \hat{\tau}_{1t}=Y_{1t}- \hat{Y}_{1t}^N
\end{equation}


Nonnegative weights ensure a convex combination of the pre-treatment values of donor 
units, so that the resulting control can be interpreted as a weighted average 
of the control units with typically sparse weights. 
Furthermore, it ensures comparability of the outcome variable by giving the 
synthetic control outcome the same scale of the intervention unit. 
Abadie (2021) notes that, when using weights that sum to one, variables in the data 
should be rescaled to correct for differences in size between the units, for 
instance by using per capita GDP instead of level GDP. This correction is not 
needed if variables are already comparable, for instance in the case of prices.
Allowing for weights outside of the $[0,1]$ interval may provide a more 
accurate synthetic control by placing negative emphasis on some donor units that 
are dissimilar to the treated one. However, negative unbounded weights may 
introduce extrapolation, where the assigned weights are used to extrapolate 
beyond the observed rage of data to estimate the effect of treatment. 
This can lead to biased estimates and reduced precision, and makes the interpretation 
of weights less straightforward. 

The core of the SCM estimation lies in the definition of the weights. 
The approach in Abadie et al. (2021) is to choose weights $w_2,...,w_{J+1}$ such that 
the synthetic unit resembles pre-treatment values of the treated unit in terms 
of the outcome variable.
Given the nonnegative constant vector $\mathbf{V}=(v_1,...,v_k)$ 
that represent the relative importance of the $k$ predictors, the optimal 
weight vector $\mathbf{W}^*$ is the one that, for some positive constants $v_h$, minimizes:
\begin{equation}
    || \mathbf{X}_1 - \mathbf{X_0} \mathbf{W} || = 
\left( \sum_{h=1}^k {v_h \left( X_{h1}-w_2 X_{h2}-\ldots - w_{J+1} 
X_{hJ+1} \right) ^2} \right)^{1/2}
\end{equation}

Subject to $\sum_{j=2}^{J+1}w_j$ and $w_j \geq 0$. That is, minimizing the distance 
between the treated unit $\mathbf{X}_1$ and the weighted combination of the 
control units $\mathbf{X_0} \mathbf{W}$. The output $\mathbf{W}^*=
\left( w_2^*,...,w_{J+1}^*\right) '$ is used in the estimation of the treatment 
effect for the treated unit across $t=T_0+1,...,T$ as:
\begin{equation}
    \hat{\tau}_{1t} = Y_{1t} - \sum_{j=2}^{J+1}{w^*_j Y_{jt}}
\end{equation}


Any possible choice of weights in $\mathbf{V}$ produces a different set of optimal 
weights, which is effectively a different synthetic control 
$\mathbf{W}(\mathbf{V})=$ \newline $\left(w_2(\mathbf{V}),...,w_{J+1}(\mathbf{V})\right)'$.
For this reason the choice of $\mathbf{V}$ is a key issue. An initial approach could 
be to divide the predictor weights equally across the $k$ predictors included in the 
model. However, two more elegant solutions are proposed by the authors.


The proposed solution is to choose $\mathbf{V}$ such that the synthetic 
control $\mathbf{W}(\mathbf{V})$ minimizes the mean squared prediction error (MSPE) 
of the synthetic control with respect to $Y_{1t}^N$ over the pre treatment period
$\mathcal{T}_0 = 1,..., T_0$ : 
\begin{equation}
    \sum_{t\in\mathcal{T}} {\left(
    Y_{1t}-w_2 \mathbf{(V)} Y_{2t}-...-w_{J+1}(\mathbf{V})Y_{J+1t}
\right)} ^2
\end{equation}

% OUT OF SAMPLE EXPLANATION
We could choose $\mathbf{V}$ via out-of-sample validation, which requires substantial 
pre-treatment observations. This exploits the observed pre treatment $Y_{1t}^N$
to gauge the predictive power of the variables $X_{1j},...,X_{kj}$ and assign 
coherent weights $\mathbf{V}$. To use out-of-sample validation start by 
dividing the $\mathcal{T}_0$ period in a \emph{training} period and a 
\emph{validation} period. The lenghts of the two periods may depend on data 
availability and frequency of measurement of outputs and variables. Then, for 
every value $\mathbf{V}$, compute the synthetic control weights on the training 
period and call them $\tilde{w}_2 \mathbf(V), ..., \tilde{w}_{J+1} \mathbf(V)$.
The MSPE over the validation period will be:
\begin{equation}
    \sum_{t=t_1+1}^{T_0} \left(
    Y_{1t}-\tilde{w}_2 \mathbf(V) Y_{2t} - ... - 
    \tilde{w}_{J+1} \mathbf(V) Y_{J+1t} \right)^2
\end{equation}

Then, compute $\mathbf{V}^* \in \mathcal{V}$ such that MSPE is minimized over this 
training period, with 
$\mathcal{V}$ being the set of all potential $\mathbf{V}$. Then, check the synthetic 
control's ability to emulate the treated unit behavior on the remaining validation 
period observations by using $\mathcal{V}^*$ to compute $\mathcal{W}^*=\mathcal{W} 
\mathcal(V)^*$.
\newline For an analysis of the shortcomings of cross-validation as defined in this 
section, see subsection 2.3.



\subsubsection{Bias properties of SCM}

Appendix B of Abadie et al. (2010) provides an analysis of the bias properties of synthetic 
control estimators in the case of a linear factor model and a vector autoregressive 
(VAR) model. According to their findings, the estimator's bias in a factor model 
scenario can be constrained by a function that goes to zero as the amount of 
pre-treatment periods increases. In an VAR scenario, the synthetic control 
estimator is unbiased.

\textcolor{red}{maybe add something more detailed if you have time} %add more






\subsubsection{Inference and robustness checks in SCM}
\textcolor{red}{Talk about placebo studies and robustness checks. Check Barone Mocetti} %complete



\subsection{A detour on methodological advantages}

SCM come with advantages relative to other competing methods. In this subsection, 
these features are emphasized with respect to linear regression estimators. 

\emph{Transparency and goodness of fit.} While both 
synthetic control estimation and regression estimators are applied to panel data, 
SCM are transparent relative to the discrepancy between the treated unit and the 
synthetic unit. Furthermore, the goodness of fit of the synthetic control unit 
can be easily evaluated through analysis of the pre tratment period. SCM should 
not be used if the fit in the pre treatment period is not satisfactory.

\emph{Extrapolation prevention.} Another advantage of synthetic controls comes from the 
restrictions placed on the weights, which prevent extrapolation. Instead, 
regression weights may lie outstide of the $[0,1]$ interval (for a comparison 
of the outcomes of SCM and regression see Abadie, Diamond and Hainmueller (2015)). 

\emph{Discourages specification searching.} Synthetic control weights can be computed 
before the observation of post-treatment outcomes, so that a researcher may decide 
on the design of the study without knowing how it would affect the conclusions.

\emph{Weight sparsity.} When estimating weights for 
the control group, regression estimators typically provide non zero weights to all control 
units. Instead, sythetic control weights are sparse. The contribution of each 
donor unit is straightforward and allows for a geometric interpretation of the 
sparse weights: the sythetic unit represents a point that lies in the convex hull 
generated by the donor units with non zero weights.
Note that with many treated units the weights are not necessarily unique nor 
that sparse. Abadie and L'Hour (2019) offers a penalized version of SCM that 
provides unique and sparse weights under some conditions.



\subsection{Contextual and data requirements for a credible application}

Most of the data and contextual requirements for successful application of 
SCM are applicable to any other comparative case study. Following Abadie (2021), I 
will start from contextual requirements, and eventually move on to data requirements.


\subsubsection{Contextual requirements}

The first observation is on the volatility of outcomes. Excessive random noise in the 
outcome variable increases the likelihood of over-fitting, therefore it is advised to 
filter out such noise before implementation, otherwise the effect of treatment 
may be drowned out. This concern does not come from common volatility between the 
units, but from unit specific volatility.

The second observation pertains to the donor pool. For a unit to be a suitable 
candidate of the donor units, it should not be affected by or subject to the 
treatment provided to the unit of interest, and it should have common features 
with the treated unit (typically, units in a similar region and/or context). 
This entails the elimination from the 
donor pool of any unit which may have suffered a shock to the outcome that would 
not have happened in the absence of treatment in unit of interest.


Third, a typical concern is anticipatory behavior by the units which would 
introduce bias in the SCM estimates. Depending on data avilability, a solution 
would be to backdate the period that marks the start of treatment. Since SCM allow 
for differential exposure to treatment across time, the initial periods that 
are barely affected by treatment may show very small effects, whereas the subsequent 
periods will show larger effects.

Spillover effects are another point of concern that stems from the selecion of 
an ideal donor unit. Usually, a donor unit is valid if it has common features 
with the treated one, while also being unaffected by the shock. As a consequence, 
donor units tend to come from the same regions or be exposed to the same 
context. Spillover effects are especially an issue if units come from the same 
geographical area. In order to understand the bias introduced by interference, 
the researcher may carry out and compare the estimates with and without the 
affected donor units. Moreover, due to the transparency of the synthetic 
counterfactual and the sparsity of weights, the researcher may reason as to the 
potential direction of the bias and account for that in their analysis.

\subsubsection{Data requirements}

For the method to accurately and credibly track the treated unit there must be 
availability of a window of pre-treatment observations from all units. Using large 
periods of time may present the issue of structural breaks in the pre-treatment 
span, affecting the structural stability of the model. 
Therefore, we must be aware of how using too long a window may compromise the 
predictive ability of the synthetic unit. 

Extensive post-treatment information is crucial for the evaluation of the effects, 
especially if they are expected to intensify or dissipate over time.






\section{Effectiveness of SCM: challenges and remedies}

Similar to any other technique, the application of synthetic controls demands 
careful consideration of the proper data and contextual prerequisites that must be 
fulfilled, along with fulfilling the required conditions for its use to claim a 
causal explanation of outcomes. Various features are currently being debated and 
enhanced in the literature, while some areas remain unexplored. 
For instance, such an area is the investigation of computational aspects, 
which is still relatively under-researched and holds significant potential for 
improvement.



\subsection{Kaul et al. (2015) on the inclusion of lagged dependent variables}

The predictors in a synthetic control application can be lagged outcome variables and other 
economically relevant variables with explanatory power for the dependent. Generally, 
the lagged outcomes tend to have greater predictive power than the covariates included 
in the model. 
Although the intuitive approach would be to include all pre-treatment lagged outcome 
variables and all the covariates, doing so could nullify the effect of the covariates 
for both nested and regression optimization approaches. The potental bias arising 
from this mechanic is something to be mindful of when considering what to include 
in the model.

To illustrate this, write predictor matrices as:
\begin{equation}
    X_1 = \left( \begin{matrix} C_1 \\ Z_1 \end{matrix} \right), \;
    X_0 = \left( \begin{matrix} C_0 \\ Z_0 \end{matrix} \right)
\end{equation}

Where $C_i$ represent covariates, and $Z_i$ represent the dependent variable elements.
In a similar manner, rewrite matrix $V$ as:
\begin{equation}
    V = \begin{pmatrix}
        V_C & 0 \\
        0 & V_Z
    \end{pmatrix}
\end{equation}
So that $V_C$ contains the covariate weights, and $V_Z$ contains the lagged dependent 
variable weights.

For given predictor weights $V$, we can write the inner optimization to find $W(V)^*$ as:
\begin{equation}
    \min_{W} \sqrt{(X_1-X_0 W)' V (X_1-X_0 W)}
\end{equation}
And the outer optimization to find the optimal $V$ that minimizes the MSPE as:
\begin{equation}
    \min_V (Z_1 - Z_0  W^*(V))' (Z_1 - Z_0 W^*(V))
\end{equation}
If we denote $W^{**}$ as:
\begin{equation}
    W^{**} := \underset{W}{\arg\min}  (Z_1 -Z_0 W)' (Z_1-Z_0 W) 
\end{equation}
Then, for any $V$ we would have:
\begin{equation}
    (Z_1-Z_0 W^{**})' (Z_1-Z_0 W^{**}) \le (Z_1 - Z_0  W^*(V))' (Z_1 - Z_0 W^*(V))
\end{equation}
However, choosing $V$ such that $V_C$ is zero and $V_Z$ is the identity matrix yields:
\begin{equation}
    W^* (V^*) = W^{**}
\end{equation}

The implication is that the outer optimization would want null predictor weights 
even if the inner optimization would assign them positive weights. Thus, the synthetic 
control would end up ignoring the covariates. Consequently, even if positive $V$ elements 
are reported, they are effectively ignored. Bias emerges from this dynamic if the 
covariates are relevant explanatory elements of the dependent. To solve for this, they 
advise to exclude outcomes lags in the inner optimization so that positive weights 
emerge for other predictors.

The authors illustrate this by replicating a paper by Billmeier and Nannicini (2013), 
and highlight a stark difference in estimated treatment effects when using all 
pre-treatment dependent varibles observation as opposed to not doing so. 

This dynamic may be exploited to understand the predictive power of predictors via the 
comparison of a full lagged dependent variable model and a reduced lagged dependent 
variable one. The discrepancy between results is indicative of whether to include 
such covariates.




\subsection{Klößner et al. (2018) on the uniqueness of weights when utilizing cross-validation techniques}

Some authors noticed numerical instability in the assigned predictor weights 
when replicating results from Abadie and Gardeazabal (2003). 
Klößner et al. (2018) investigate this ambiguity by looking at validation weights. 
When using the out-of-sample validation technique, 
predictor weights are not necessarily uniquely defined, so that when replicating 
Abadie et al. (2015) the authors find different yet equivalent solutions 
for the weights depending on the software package used (specifically, R with 
the SYNTH package versus STATA) and on the specific donor units ordering (alphabetical 
versus custom).

Cross-validation requires the sectioning of the pre-treatment observations 
in a training period and validation period, and a two-step estimation procedure 
that computes predictor and donor weights. This is opposed to the standard 
one step estimation procedure of SCM and generates ambiguity on weights. 
The authors develop a rule of thumb to gauge the extent of this ambiguity 
in the estimates of the treatment effect that is centered on the difference 
$k-\alpha$, where $k$ is the number of predictors and $\alpha$ is the number of 
donor units that obtains positive weights in the training period.
If $k-\alpha>0$, the predictor weights are not unique and there will be 
proportionally increasing ambiguity.

On the other hand, the one-step procedure to estimate donor weights $W^*(V^*)$ 
provides a well-defined estimator that outputs unique donor weights. The authors 
stress that the ill-defined cross-validation synthetic control estimator is no 
failure of the method as such, and that the ambiguity may be negligible, although 
a positive $k-\alpha$ is the typical scenario.




\subsection{Kuosmanen et al. (2021) on the true nature of numerical instability}

In contrast with the findings from Klößner et al. (2018) that point to the 
cross-validation method as culprit, Kuosmanen et al. (2021) ascribe numerical 
instability to the commonly used \emph{Synth} package algorithm available for 
R, STATA and Matlab and the MSCMT algorithm which produce unstable and suboptimal 
weights that to converge to the existing optimum unique solution. 
Therefore, different ordering of the donors and predictors affect the results. 

The authors stress that numerical instability is not the underlying flaw, but more 
a symptom. They show that the model has a tendency for corner solutions that 
is the true design flaw and is caused by joint optimization of donor and 
predictors weights. The computational complexity of what is an NP-hard 
bilevel optimization problem causes other packages to fail. 
Their proposed solution is to determine predictor and donor 
weights by applying a two-step algorithm that optimizes the donor 
weights when the predicor weights are given (note that the \emph{Synth} package 
fails to derive optimal weights even when predictor weights are given), 
an approach that builds on previous work by Malo et al. (2020). The
authors provide the updated code and technical documentation for this approach 
at their GitHub page (see p.4 of Kuosmanen et al. (2021) for the link).

An explicit restatement of the SCM problem provided by Malo et al. (2020) 
reveals the computational difficulty of problem. Following the paper's 
notation:
% mathematical formal restatement of the problem by Malo et al. (2020)
\begin{equation}
    \min_{\mathbf{v,w}} L_V = 
    \frac{1}{T^{pre}} \left(y_1^{pre}-Y_0^{pre}w)\right) ' 
    \left( y_1^{pre}-Y_0^{pre}w \right)
\end{equation}

subject to
\begin{equation}
    \mathbf{w}= argmin L_W = \left( x_1-X_0 w \right) ' \mathbf{V}
    \left( x_1 - X_0 w\right)
\end{equation}
$$\mathbf{1'w}=1$$
$$\mathbf{1'v}=1$$
$$\mathbf{w}\ge 0, \mathbf{v} \ge 0$$
Where $(15)$ is the outer optimization problem, and $(16)$ is the inner 
optimization problem. $T^{pre}$ is the pre intervention period. This could be 
interpreted as a social planner playing a Stackelberg game where the 
leader chooses $\mathbf{v}$ and the follower chooses $\mathbf{w}$ cooperatively.
\newline
The proposed solution is detailed next.

\emph{Step 1}: Solve the quadratic optimization problem.
$$
    \min_w  L_Q = (x_q-X_0 w)' V^* (x_1 - X_0 w)
$$
subject to 
$$ \mathbf{1'w=1} $$
$$ \mathbf{w \ge 0} $$

\emph{Step 2}: Given the step 1 optimal solution for $L_W^*$, solve the 
convex optimization problem.
$$ \min_w  L_V = (y_1^{pre}-Y_0^{pre}w)'(y_1^{pre}-Y_0^{pre}w)$$
subject to
$$ \mathbf{1'w=1} $$
$$ \mathbf{w \ge 0} $$
This procedure allows for multiple optima in Step 1, unlike \emph{Synth}.




\subsection{Ferman et al. (2020) on specification searching opportunities}

The authors pose the issue of specification searching behavior in synthetic 
control applications stemming from a lack of consensus about which specifications should 
be used in SCM applications which has lead to a wide variety of choices in the 
literature. If different model specifications lead to different synthetic control 
units, the researcher may be tempted to discretionarily select the ones that lead 
to stastically significant results. Via Monte Carlo and placebo simulations they 
find that the probability of detecting a false positive is decreasing in the 
number of available pre-treatment periods, but still large when the time window 
is much greater than what is usually used in the literature.

They explore opportunities for specification searching when selecting the number 
of pre-treatment periods to include in the model. In accordance with Kaul et al.
(2015), they suggest to not include all pre-treatment periods if the model 
includes relevant covariates. If there are no covariates, the inclusion of all 
pre-treatment periods is the MSPE minimizing choice. The recommendation is to 
always include multiple sepcifications in synthetic control applications, and 
use the full pre-treatment period model as a benchmark.

A commonly used criterion to choose between different specifications is to 
select the ones that minimize the MSPE over the validation period. Some examples 
of this in practice are Dube and Zipperer (2015) and Donohue, Aneja and Weber 
(2019).




\subsection{Klößner and Pfeifer (2017) on using SCM as a forecasting techinque}
Although the computation of a synthetic control does not strictly require post treatment
values, not much work has been done on the use of SCM as a forecasting technique.
In fact, Klößner and Pfeifer (2017) is the only academic work I could find on the 
topic and is mostly an introduction to the possibility of using SCM in this context. 
The authors compare the performance of SCM in forecasting US one period GDP growth 
to other well-known forecasting methods such as Holt-Winters, Random Walk and 
ARMA(p,q) with $p,q\in\{ 0,1,2,3,4\}$ are chosen with Akaike ans Bayes's information 
criteria.

The authors introduce a discount factor $\beta$ in the outer optimization problem 
so that more recent observations weigh more:
\begin{equation}
    \min_{V} 
    \sum_{t=1}^{T} {\beta^{T-t} \left( Y^1_t - 
    \sum_{j=2}^{J+1} {W^*(V)_j Y^j_t} \right)^2}
\end{equation}

The inner optimization remains unchanged. 
In the model, the aim is to forecast the first quarter of 2015 by using as 
potential donors the lags of US GDP going 
from $1, \dots, H$, where $H\in\{8,12,16,20,24,28,32,36,40\}$ is 
the maximum lag included. No covariates are included in the model. 

To compare the quality of the forecast, the authors use different criteria, among 
which we find an average rank of the estimator according to forecast precision,
mean absolute prediction error (MAPE) and root mean squared error (RMSPE).
They find that RMSPE slightly favours SCM over other methods. When SCM is the 
winner, a higher $\beta$ results in more precise estimates. Overall, SCM performs 
competitively with respect to other methods. 












\section{Applying SCM to the analysis of extreme natural events}
Synthetic control methods have been applied to the study of right-to-carry laws, 
legalized prostitution, immigration policy, corporate connections, minimum wages and 
much more. This section focuses on reviewing the literature that applies synthetic 
control methods to the investigation of natural disasters and extreme natural 
events. For each of the reviewed works, I will provide a description the data 
and of which variation of SCM has been applied, as well as a brief review of 
the outcomes. 



\subsection{Cavallo et al. (2013) on catastrophic natural disasters and economic growth}
This paper looks for empirical evidence on whether natural disasters affect economic growth.
Economic theory does not provide a clear cut answer to the question, with neoclassical 
growth model stating that the destruction of capital following a natural disaster should 
not affect the rate of technological progress, hence it should only affect short term 
growth, and endogenous growth models even stating that growth could be higher as a result 
of reinvestment and upgrading of capital (think of the Schumpeterian creative destruction).

\subsubsection*{Strategy}
For each country affected by a natural disaster, the authors compute a counterfactual 
via SCM. The synthetic controls are computed from data about unaffected countries with 
the aim of producing a counterfactual for a country affected by a large not recurring 
event. Only disasters before the year 2000 are considered.

The model is applied on a sample of $J+1$ countries, where the first country $J=1$ is 
affected by the disaster and the remaining $J$ countries are the donor pool. The aim is 
to estimate $$\alpha_{it} = Y^I_{it} - Y^N_{it}$$ where $Y^I_{it}$ is GDP per capita 
for country $i$ at time $t$ if the country is exposed to the disaster, and $Y^N_{it}$ 
is GDP per capita for country $i$ at time $t$ in the absence of a disaster. 
While $Y^I_{1t}$ is observed, we need to find an estimate for $Y^N_{1t}$. 
Following Abadie et al. (2010), they proceed by estimating $W^*$ and $V^*$ such that 
the MSPE is minimized using the out-of-sample validation technique. 


\subsubsection*{Data}
The authors use a dataset covering 196 countries between 1970 and 2008 which includes:
\begin{itemize}
    \item Data on real GDP per capita and purchasing power parity (PPP) from the World 
    Bank and World Development Indicators (WDI).
    \item GDP predictors for:
    \begin{itemize}
        \item Trade openness from WDI;
        \item Capital stock from Penn World Tables (PWT);
        \item Land area in km$^2$;
        \item Population;
        \item Secondary educational attainment from Lutz et al. (2007);
        \item Latitude in absolute value;
        \item Polity 2 indicator from the Polity IV database.
    \end{itemize}
    \item Data on natural disasters, their human and economic impacts from the EM-DAT 
    database collected by the Centre for Research on the Epidemiology of Disasters (CRED). 
    This database has information on natural disasters from 1900 at the condition that 
    at least 10 or more people have died, 100 people have been affected, a state of 
    emergency is declared or international assistance is called for. It provides two measures 
    of the magnitude of a disaster: number of people killed and amount of direct damage in 
    US dollars.
     
\end{itemize}

The authors only focus on large disasters, defined as exceeding the cutoffs of 99th, 90th 
and 75th percentiles of the world distribution of number of people killed. The 99th percentile 
corresponds to more than 233 deaths per 1 million inhabitants, the 90th to 17 deaths per 1 
million, and the 75th to 7 deaths per 1 million (the mortality of Hurricane Katrina in 2005).
Using these cutoffs, the subsample of diasters happening before 2000 with full data are of 4 
events for the 99th percentile cutoff, 18 for the 90th percentile and 22 for the 74th percentile.


\subsubsection*{Results}
Findings show statistically significant and negative effects of very large natural disasters 
on GDP growth for both the short and long term. 
Of the four very large disaster (99th percentile cutoff), the 1979 Islamic Iranian Revolution 
after the 1978 earthquake and the Sandinista Nicaraguan Revolution of 1979 that followed an 
earthquake were followed by political revolutions.
When controlling for political upheaval, the significance disappears. This dynamic is reminiscent 
of Acemoglu, Johnson and Robinson's (2005) work on the significance of institutions as a 
determinant of economic growth.
For countries unaffected by revolutions and/or milder natural events, the effect of disasters 
is not significant. 

As noted in Section 2.4, the interpretation of SCM as causal rests on the assumption that 
countries from the donor pool are unaffected both directly and indirectly by the treatment for 
unit $J=1$. It is very hard to rule out this type of interaction if the donor pool has relations 
with the treated unit, so the authors check that the control group is not affected by the 
presence of a main trading partner.



\subsection{Barone, Mocetti (2014) on growth and institutions after earthquakes in Italy}
Barone and Mocetti essentially apply Cavallo's et al. (2013) approach to the case of two 
earthquakes in Italy, with the stated advantage that they can exactly pinpoint the geographical 
areas affected by the events instead of considering an entire country as impacted. 
A second advantage comes from the better comparability of Italian 
regions rather than comparing different countries. Third, they use data that is uniformly built 
since it comes from the same institutional sources. Finally, they can control for the amount of 
public transfers to each region, which play a crucial role in the long term consequences of 
natural disasters depending on whether these resources are misallocated or used to enchance the 
local economy.

\subsubsection*{Strategy}
The empirical strategy is the one from Abadie et al. (2010) applied to $J+1$ Italian regions, of 
which the first unit is the treated one. 
Weights $W$ are chosen to minimize the MSPE function. 



\subsubsection*{Data}
Data for the main outcome variable GDP per capita and a 
number of predictors (labor units, invesments, population, sectoral shares of value added) comes 
from the CRENOS research institute and covers the 1951-2004 period. Then, the same data for 
2004-2009 comes from the National Statistical Institute (ISTAT). Human capital data comes from 
ISTAT's census information.

The authors build four regional time-varying variables to control for the quality of 
local institutions. The first uses ISTAT data on embezzlement, bribery, corruption, infringement 
of public duty, fraud, fencing and bankruptcies to measure the intensity of corruption and 
fraudulent behavior. Then, using data from Fondazione Rodolfo de Benedetti, they build the fraction 
of national memebers of parliament appointed in each region who were involved in scandals. 
The third variables captured political participation by using referenda and European election 
turnout for each region. The final one is the diffusion of newspaper readership using data 
from Accertamenti Diffusione Stampa (ADS).

Data on the number of deaths and affected people comes from the aforementioned EM-DAT. The 
first earthquake in the Friuli region affected killed 922 and 218,200 people, the second one 
in Irpinia (Campania and Basilicata regions) affected 407,700 and killed 4689.

\subsubsection*{Results}
The method attaches sparse weights to a small number of donor pool regions in both cases, as usual.
The main results are the absence of a significant short term shock. Accounting for fiscal stimulus 
in the form of financial aid highlights that it represented a sizeable chunk of regional GDP, so much 
so that without financial aid, in the five years following the disasters, Friuli's GDP per capita would 
have been 2-4\% lower and Irpinia's would have been 6-11\% lower. However, in the long term the synthetic 
control detects two divergent trends of GDP per capita in the two regions. This can be reconnected to 
the quality of institutions and the consequent use of financial aid resources, with Friuli steering 
toward virtuous reconstruction, and Irpinia experiencing more rent seeking behaviors and misallocation 
of resources.  

Financial aid designed in a way that is conscious of the institutional quality and local dynamics 
favours immediate reconstruction and cushions the negative effects on GDP.



\subsection{Coffman, Noy (2011) on measuring the long term impact of Hurricane Iniki}
Coffman and Noy study the long term impact of the 1992 category 4 hurricane Iniki on 
the Hawaian county of Kauai, using the unaffected islands as a donor pool to build 
a synthetic control. 
What makes the counties ideal candidates for the application of SCM is that they share 
similar economic structures, are affected by common shocks, they belong to the same 
state and thus share institutions and legal frameworks. 
Differences between the counties are mostly in terms of size.
At the time of publication, Iniki was the strongest hurricane to hit Hawaii in recorded 
history causing \$7.4 billion of direct damages at 2008 prices.

\subsubsection*{Strategy}
The SCM is applied as in Abadie et al. (2010) with regression methods. %finish this
To account for potential indirect effects of Iniki on the economic activity of other 
counties, Maui was excluded from the donor pool. This is due to tourism being the 
main economic activity of all islands and to Maui temporarily absorbing arrivals to 
the Kauai county right after the hurricane hit. 
Another channel for indirect effects could be that of displacement of residents to 
other islands, however data shows this is not an issue. 

\subsubsection*{Data}
Data is collected from 1975 to 2011, and inlcudes information on employment, resident 
population, personal income, per capita income and transfer payments.  
Once again, data on deaths (4) and affected people (25,000) comes from the EM-DAT 
database.

\subsubsection*{Results}
Placebo testing confirms two central results. 
First, Kauai's population trajectory took a hit following the hurricane, and kept 
growing slowly in the following years, with a permanent loss of 12\% of its population.
Second, private sector employment also declined significantly, with a permanent loss 
of around 15\% of the total county employment. 




\section{Applying SCM to policy evaluation}
This section is dedicated to a literature review focused on policy evaluation related 
to emissions, international agreements and event studies. Analogously to the previous 
section, I will focus on strategy, data sources and results for each work. 


\subsection{Cole, Elliott, Liu (2020) on the impact of lockdowns on Wuhan air pollution and health}
This paper by Cole et al. applies a combination of machine learning 
to clean weather data and a variation of SCM called augmented SCM to study the effect 
of reduced air pollution during lockdown on health. 
Improvements in air quality and associated health benefits are critical points in 
elaborating climate change related policy and emissions regulation. 


\subsubsection*{Strategy} %complete
Since concentrations of pollutants depend on local weather conditions, the comparison 
of emissions across time is difficult. As a solution, the authors adopt a machine 
learning approach that allows to remove the effect of weather conditions on concentrations 
of pollutants. 
The city of Wuhan is one of 17 large cities in its province of Hubei (China), and it 
implemented a lockdown at least two weeks in advance with respect to other cities. 

\subsubsection*{Data}
The Qingyue Open Environmental Data Center provides data on sulphur dioxide (SO$_2$), 
nitrogen dioxide (NO$_2$), carbon monoxide (CO) and particulate matter (PM10) between 
January 2013 and February 2020 for thirty Chinese cities. 

\subsubsection*{Results}



\subsection{Steinkraus (2018)}
Text

\subsubsection*{Strategy}
\subsubsection*{Data}
\subsubsection*{Results}



\subsection{Andersson (2019)}
Text

\subsubsection*{Strategy}
\subsubsection*{Data}
\subsubsection*{Results}



\subsection{Maamoun (2019)}
Text

\subsubsection*{Strategy}
\subsubsection*{Data}
\subsubsection*{Results}



\subsection{He et al. (2022)}
Text

\subsubsection*{Strategy}
\subsubsection*{Data}
\subsubsection*{Results}



\section{Potential applications to Fleurbeay project}
\textcolor{red}{Lorem ipsum}





\section{Conclusion}
\textcolor{red}{Lorem ipsum}


\newpage





\section{References}
\begin{itemize}
    \item Abadie, Alberto, Alexis Diamond, and Jens Hainmueller. ``Synthetic control methods for comparative case studies: Estimating the effect of California's tobacco control program." Journal of the American statistical Association 105.490 (2010): 493-505.
    \item Abadie, Alberto, and Javier Gardeazabal. ``The economic costs of conflict: A case study of the Basque Country." American economic review 93.1 (2003): 113-132.
    \item Abadie, Alberto. ``Using synthetic controls: Feasibility, data requirements, and methodological aspects." Journal of Economic Literature 59.2 (2021): 391-425.
    \item Abadie, Alberto, and Jérémy L'hour. ``A penalized synthetic control estimator for disaggregated data." Journal of the American Statistical Association 116.536 (2021): 1817-1834.
    \item Acemoglu, Daron, et al. ``The value of connections in turbulent times: Evidence from the United States." Journal of Financial Economics 121.2 (2016): 368-391.
    \item Acemoglu, Daron, Simon Johnson, and James A. Robinson. ``Institutions as a fundamental cause of long-run growth." Handbook of economic growth 1 (2005): 385-472.
    \item Athey, Susan, and Guido W. Imbens. ``The state of applied econometrics: Causality and policy evaluation." Journal of Economic perspectives 31.2 (2017): 3-32.
    \item Barone, Guglielmo, and Sauro Mocetti. ``Natural disasters, growth and institutions: a tale of two earthquakes." Journal of Urban Economics 84 (2014): 52-66.
    \item Ben-Michael, Eli, Avi Feller, and Jesse Rothstein. ``The augmented synthetic control method." Journal of the American Statistical Association 116.536 (2021): 1789-1803.
    \item Cavallo, Eduardo, et al. ``Catastrophic natural disasters and economic growth." Review of Economics and Statistics 95.5 (2013): 1549-1561.
    \item Coffman, Makena, and Ilan Noy. ``Hurricane Iniki: measuring the long-term economic impact of a natural disaster using synthetic control." Environment and Development Economics 17.2 (2012): 187-205.
    \item Cole, Matthew A., Robert JR Elliott, and Bowen Liu. ``The impact of the Wuhan Covid-19 lockdown on air pollution and health: a machine learning and augmented synthetic control approach." Environmental and Resource Economics 76.4 (2020): 553-580.
    \item Doudchenko, Nikolay, and Guido W. Imbens. ``Balancing, regression, difference-in-differences and synthetic control methods: A synthesis." No. w22791. National Bureau of Economic Research, 2016.
    \item Dube, Arindrajit, and Ben Zipperer. ``Pooling multiple case studies using synthetic controls: An application to minimum wage policies." (2015).
    \item Donohue, John J., Abhay Aneja, and Kyle D. Weber. ``Right-to-carry laws and violent crime: A comprehensive assessment using panel data and a state‐level synthetic control analysis." Journal of Empirical Legal Studies 16.2 (2019): 198-247.
    \item Luo, Kevin, and Tomoko Kinugasa. ``Do natural disasters influence long-term savings?: Assessing the impact of the 2008 Sichuan earthquake on household saving rates using synthetic control." China: An International Journal 18.3 (2020): 59-81.
    \item Malo, Pekka, et al. ``Computing Synthetic Controls Using Bilevel Optimization." (2020).
    \item Kaul, Ashok, et al. ``Synthetic control methods: Never use all pre-intervention outcomes together with covariates." (2015).
    \item Klößner, Stefan, et al. ``Comparative politics and the synthetic control method revisited: A note on Abadie et al.(2015)." Swiss journal of economics and statistics 154 (2018): 1-11.
    \item Klößner, Stefan, and Gregor Pfeifer. ``Outside the box: Using synthetic control methods as a forecasting technique." Applied Economics Letters 25.9 (2018): 615-618.
\end{itemize}




\end{document}